{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2.12.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pydantic) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pydantic) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from pydantic) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers huggingface_hub accelerate\n",
    "# !pip install soundfile transformers\n",
    "!pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soundfile\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from soundfile) (2.0.0)\n",
      "Requirement already satisfied: numpy in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from soundfile) (1.26.4)\n",
      "Requirement already satisfied: pycparser in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from cffi>=1.0->soundfile) (2.23)\n",
      "Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: soundfile\n",
      "Successfully installed soundfile-0.13.1\n"
     ]
    }
   ],
   "source": [
    "!pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch<3,>=2.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from bitsandbytes) (2.8.0+cu128)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from bitsandbytes) (25.0)\n",
      "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (2025.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
      "Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m147.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.48.2\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "torch.cuda.empty_cache()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers.generation.utils\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbeab9fdd22743e1ae8b2fa2b2806110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': device(type='cuda')}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"NCAIR1/N-ATLaS\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device,\n",
    "    dtype=torch.bfloat16\n",
    "\n",
    ")\n",
    "\n",
    "print(model.hf_device_map)\n",
    "\n",
    "\n",
    "\n",
    "def format_text_for_inference(messages):\n",
    "    current_date = datetime.now().strftime('%d %b %Y')\n",
    "    text= tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "        date_string=current_date\n",
    "    )\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_english(user_prompt):\n",
    "    sys_instructions = \"\"\"\n",
    "        You are an English grammar expert.\n",
    "        Rewrite the user's input into clear, correct, standard English.\n",
    "        Preserve the meaning.\n",
    "        Output ONLY the corrected sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    q_chat = [\n",
    "        {'role':'system','content': sys_instructions},\n",
    "        {'role': 'user', 'content': user_prompt}\n",
    "    ]\n",
    "\n",
    "    text = format_text_for_inference(q_chat)\n",
    "\n",
    "    input_tokens = tokenizer(text,return_tensors='pt',add_special_tokens=False).to(device)\n",
    "    outputs = model.generate(\n",
    "        **input_tokens,\n",
    "        max_new_tokens = 200,\n",
    "        use_cache=True,\n",
    "        temperature = 0.01\n",
    "    )\n",
    "\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "    # extract assistant response\n",
    "    pattern = r\"<\\|start_header_id\\|>assistant<\\|end_header_id\\|>\\s*(.*?)<\\|eot_id\\|>\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return user_prompt  # fallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am going to school today.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_english(\"I go to school today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ọ̀gbẹ́ni Wọlé ti jẹun.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"Mr. Wole has eaten\", \"Yoruba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(user_prompt, language):\n",
    "    sys_instructions = f\"\"\"\n",
    "        You are language expert and teacher in the following language: [{language}] and English. You are to\n",
    "        follow the following instructions:\n",
    "        \n",
    "        1. Your role is to translate the user prompt from English to the given language {language}\n",
    "        2. Be precise as much as possible.\n",
    "        3. Do not give any explanations, just give answers\n",
    "\n",
    "            \"\"\"\n",
    "    \n",
    "    user_prompt = correct_english(user_prompt)\n",
    "    q_chat = [\n",
    "        {'role':'system','content': sys_instructions},\n",
    "        {'role': 'user', 'content': user_prompt}\n",
    "    ]\n",
    "\n",
    "    text = format_text_for_inference(q_chat)\n",
    "\n",
    "    input_tokens = tokenizer(text,return_tensors='pt',add_special_tokens=False).to(device)\n",
    "    outputs = model.generate(\n",
    "        **input_tokens,\n",
    "        max_new_tokens = 1000,\n",
    "        use_cache=True,\n",
    "        repetition_penalty=1.12,\n",
    "        temperature = 0.01\n",
    "        )\n",
    "\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "\n",
    "    pattern = r\"<\\|start_header_id\\|>assistant<\\|end_header_id\\|>\\s*(.*?)<\\|eot_id\\|>\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        final_answer = match.group(1).strip()\n",
    "        return final_answer\n",
    "    else:\n",
    "        return \"Assistant response not found.\"\n",
    "\n",
    "\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, VitsModel\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "TTS_MODEL_NAME = \"facebook/mms-tts-yor\"\n",
    "tts_processor = AutoProcessor.from_pretrained(TTS_MODEL_NAME)\n",
    "tts_model = VitsModel.from_pretrained(TTS_MODEL_NAME)\n",
    "tts_model = tts_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text, output_path=\"output.wav\"):\n",
    "    # 1. Encode text\n",
    "    inputs = tts_processor(text=text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # 2. Generate audio waveform\n",
    "    with torch.no_grad():\n",
    "        outputs = tts_model(**inputs)\n",
    "        audio = outputs.waveform.squeeze().cpu().numpy()\n",
    "\n",
    "    # 3. Save audio\n",
    "    sf.write(output_path, audio, 16000)\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def translate_and_speak(user_prompt, language, tts_file=\"translation_audio.wav\"):\n",
    "    # --- 1. TRANSLATE USING YOUR NATLAS FUNCTION ---\n",
    "    translation = translate(user_prompt, language)   # <-- your existing N-ATLaS function\n",
    "\n",
    "    # --- 2. TEXT → SPEECH ---\n",
    "    audio_path = text_to_speech(translation, tts_file)\n",
    "\n",
    "    return {\n",
    "        \"translation_text\": translation,\n",
    "        \"audio_file\": audio_path\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation_text': 'Mo ji loni ni itara, titi emi o fi de ile-iwe naa, olori ise naa si wọle, o si bere soro ni ede ti ko dun lati gbọ.',\n",
       " 'audio_file': 'translation_audio.wav'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_and_speak(\"I woke up today feeling good until I got to school and the coordinator came in and he began to speak in languages that are not pleasant to the ears.\", \"Yoruba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Na tashi a yau cikin koshin lafiya, har sai da na isa makaranta kuma mai gadin ya shigo, sannan ya fara magana da harsuna masu ban tsoro.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"I woke up today feeling good until I got to school and the coordinator came in and he began to speak in languages that are not pleasant to the ears.\", \"Hausa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c72514316b343a29724ed8102f4356f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'': device(type='cuda')}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'translation_text': 'Mo ji loni ni rilara ti o dara, ṣugbọn nigba ti mo de ile-iwe, oludari naa wọle o si bẹrẹ sọ̀rọ̀ ní èdè tí kò dára fún etí.',\n",
       " 'audio_file': 'translation_audio.wav'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import warnings\n",
    "from transformers import AutoProcessor, VitsModel\n",
    "import soundfile as sf\n",
    "torch.cuda.empty_cache()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers.generation.utils\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"NCAIR1/N-ATLaS\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device,\n",
    "    dtype=torch.bfloat16\n",
    "\n",
    ")\n",
    "\n",
    "print(model.hf_device_map)\n",
    "\n",
    "\n",
    "\n",
    "def format_text_for_inference(messages):\n",
    "    current_date = datetime.now().strftime('%d %b %Y')\n",
    "    text= tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "        date_string=current_date\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def correct_english(user_prompt):\n",
    "    sys_instructions = \"\"\"\n",
    "        You are an English grammar expert.\n",
    "        Rewrite the user's input into clear, correct, standard English.\n",
    "        Preserve the meaning.\n",
    "        Output ONLY the corrected sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    q_chat = [\n",
    "        {'role':'system','content': sys_instructions},\n",
    "        {'role': 'user', 'content': user_prompt}\n",
    "    ]\n",
    "\n",
    "    text = format_text_for_inference(q_chat)\n",
    "\n",
    "    input_tokens = tokenizer(text,return_tensors='pt',add_special_tokens=False).to(device)\n",
    "    outputs = model.generate(\n",
    "        **input_tokens,\n",
    "        max_new_tokens = 200,\n",
    "        use_cache=True,\n",
    "        temperature = 0.01\n",
    "    )\n",
    "\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "    # extract assistant response\n",
    "    pattern = r\"<\\|start_header_id\\|>assistant<\\|end_header_id\\|>\\s*(.*?)<\\|eot_id\\|>\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    else:\n",
    "        return user_prompt  # fallback\n",
    "\n",
    "\n",
    "def translate(user_prompt, language):\n",
    "    sys_instructions = f\"\"\"\n",
    "        You are language expert and teacher in the following language: [{language}] and English. You are to\n",
    "        follow the following instructions:\n",
    "        \n",
    "        1. Your role is to translate the user prompt from English to the given language {language}\n",
    "        2. Be precise as much as possible.\n",
    "        3. Do not give any explanations, just give answers\n",
    "\n",
    "            \"\"\"\n",
    "    \n",
    "    user_prompt = correct_english(user_prompt)\n",
    "    q_chat = [\n",
    "        {'role':'system','content': sys_instructions},\n",
    "        {'role': 'user', 'content': user_prompt}\n",
    "    ]\n",
    "\n",
    "    text = format_text_for_inference(q_chat)\n",
    "\n",
    "    input_tokens = tokenizer(text,return_tensors='pt',add_special_tokens=False).to(device)\n",
    "    outputs = model.generate(\n",
    "        **input_tokens,\n",
    "        max_new_tokens = 1000,\n",
    "        use_cache=True,\n",
    "        repetition_penalty=1.12,\n",
    "        temperature = 0.01\n",
    "        )\n",
    "\n",
    "    text = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "\n",
    "    pattern = r\"<\\|start_header_id\\|>assistant<\\|end_header_id\\|>\\s*(.*?)<\\|eot_id\\|>\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        final_answer = match.group(1).strip()\n",
    "        return final_answer\n",
    "    else:\n",
    "        return \"Assistant response not found.\"\n",
    "\n",
    "\n",
    "def text_to_speech(text, language, output_path=\"output.wav\"):\n",
    "    if language.lower() == \"yoruba\":\n",
    "        TTS_MODEL_NAME = \"facebook/mms-tts-yor\"\n",
    "    elif language.lower() == \"igbo\":\n",
    "        TTS_MODEL_NAME = \"facebook/mms-tts-ibo\"\n",
    "    elif language.lower() == \"hausa\":\n",
    "        TTS_MODEL_NAME = \"facebook/mms-tts-hau\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported language: {language}\")\n",
    "    tts_processor = AutoProcessor.from_pretrained(TTS_MODEL_NAME)\n",
    "    tts_model = VitsModel.from_pretrained(TTS_MODEL_NAME)\n",
    "    tts_model = tts_model.to(\"cuda\")\n",
    "\n",
    "    inputs = tts_processor(text=text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = tts_model(**inputs)\n",
    "        audio = outputs.waveform.squeeze().cpu().numpy()\n",
    "\n",
    "    sf.write(output_path, audio, 16000)\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def translate_and_speak(user_prompt, language, tts_file=\"translation_audio.wav\"):\n",
    "    # --- 1. TRANSLATE USING YOUR NATLAS FUNCTION ---\n",
    "    translation = translate(user_prompt, language)   # <-- your existing N-ATLaS function\n",
    "\n",
    "    # --- 2. TEXT → SPEECH ---\n",
    "    audio_path = text_to_speech(translation, language, tts_file)\n",
    "\n",
    "    return {\n",
    "        \"translation_text\": translation,\n",
    "        \"audio_file\": audio_path\n",
    "    }\n",
    "\n",
    "user_prompt = \"I woke up today feeling good until I got to school and the coordinator came in and he began to speak in languages that are not pleasant to the ears.\"\n",
    "translate_and_speak(user_prompt, \"Yoruba\", tts_file=\"translation_audio.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
